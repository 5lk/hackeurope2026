import json
from langchain_core.messages import HumanMessage
from packages.core.src.llm_client import LLMClient
from packages.core.src.prompts import load_prompt
from packages.orchestrator.src.state import GraphState, QAResult

def qa_node(state: GraphState) -> dict:
    llm = LLMClient(model="gemini-2.5-flash", temperature=0)
    system_prompt = "You are the QA Reviewer. Review the code to ensure it meets the plan."
    
    # We want to review the generated code
    swe_results = state.get("swe_results", [])
    
    # 1. Very Fast Syntax Checking (Bypasses LLM if broken)
    import ast
    compile_errors = []
    test_files_to_run = []
    
    # Save a temporary copy to test
    import os, subprocess
    sandbox_dir = os.path.join(os.getcwd(), "output")
    os.makedirs(sandbox_dir, exist_ok=True)
    
    for r in swe_results:
        filename = r.get('target_file', '')
        code = r.get('code_snippet', '')
        
        # Save to sandbox
        clean_path = filename.lstrip("/\\")
        if ":" in clean_path:
            clean_path = clean_path.split(":", 1)[-1].lstrip("/\\")
        safe_path = os.path.join(sandbox_dir, clean_path)
        os.makedirs(os.path.dirname(safe_path), exist_ok=True)
        with open(safe_path, "w", encoding="utf-8") as f:
            f.write(code)
            
        if filename.endswith('.py'):
            try:
                ast.parse(code)
                if "test" in filename.lower():
                    test_files_to_run.append(filename)
            except SyntaxError as e:
                compile_errors.append(f"File {filename} has a SyntaxError: {e.msg} at line {e.lineno}")
                
    if compile_errors:
        err_msg = "\n".join(compile_errors)
        qa_result: QAResult = {
            "passed": False,
            "feedback": f"The code failed to compile. Fix these syntax errors:\n{err_msg}"
        }
        return {
            "qa_results": [qa_result],
            "requires_fixes": True,
            "fix_instructions": qa_result["feedback"],
            "messages": [HumanMessage(content=f"QA Review failed instantly (Syntax Error): {err_msg}")]
        }
    
    # 2. Relaxed LLM Architecture Review
    code_compilation = "\n====\n".join([f"File: {r['target_file']}\n{r['code_snippet']}" for r in swe_results])
    
    prompt = f"""
Review the following code generated by the SWE team based on the architecture plan.
Architecture Plan: {state.get('architecture_plan', {}).get('raw_response', 'No plan')}

Code:
{code_compilation}

Did the SWEs generally follow the plan? 

CRITICAL INSTRUCTIONS FOR QA:
You are reviewing code written by parallel agents who cannot see each other's exact variable names.
You MUST be extremely lenient. 
- Ignore minor variable name mismatches (e.g. `config.PLAYER_X` vs `config.START_X`).
- Ignore if they added an extra class or missed a minor feature.
- ONLY fail the review if the code is fundamentally broken, has massive syntax errors, or completely failed to generate Python code.
- If it looks like a somewhat reasonable attempt at the architecture, you MUST pass it.

Return valid JSON:
{{
    "passed": true|false,
    "feedback": "Be very brief. Empty if passed."
}}
    """
    
    try:
        data = llm.generate_json(prompt, system_prompt=system_prompt)
        
        passed = data.get("passed", True)
        feedback = data.get("feedback", "")
        
        qa_result: QAResult = {
            "passed": passed,
            "feedback": feedback
        }
        
        return {
            "qa_results": [qa_result],
            "requires_fixes": not passed,
            "fix_instructions": feedback,
            "messages": [HumanMessage(content=f"QA Review passed: {passed}. {feedback}")]
        }
    except Exception as e:
        # Fallback to true to prevent infinite loops on parsing errors
        return {
            "requires_fixes": False,
            "messages": [HumanMessage(content="QA failed to parse. Assuming passed.")]
        }
